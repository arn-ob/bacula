# bacula-dir-local.conf
#
# JobDefs
# Job
# Restore
# Pool
# Autochanger
#


# Template to store in cloud
JobDefs {
  # DefaultJobToCloudAWS
  Name = "DefaultJobToLOCAL"

  Type = Backup
  Level = Incremental
  Client = bacula-fd
  FileSet = "Full Set"
  Schedule = "WeeklyCycle"

  # CloudS3AWS
  Storage = "LOCALStorage"
  Messages = Standard

  # CloudAWS
  Pool = LocalPool

  SpoolAttributes = yes
  Priority = 10
  Write Bootstrap = "/opt/bacula/working/%c.bsr"
}

# Jobs
Job {
  # BackupClient1ToCloudAWS
  Name = "BackupClientLOCAL" 

  # DefaultJobToCloudAWS
  JobDefs = "DefaultJobToLOCAL"
}


# Cloud Pool definition
Pool {
  # CloudAWS
  Name = LocalPool
  Pool Type = Backup
  Recycle = no                        # Bacula can automatically recycle Volumes
  AutoPrune = yes                     # Prune expired volumes
  Volume Retention = 365 days         # one year
  Maximum Volume Jobs = 1             #
  # Maximum Volume Bytes = 100M          # Limit Volume size to something reasonable
  Label Format = "Vol-JobId-${JobId}"      # Auto label
}


# Autochanger definition
Autochanger {
  # CloudS3AWS
  Name = "LOCALStorage"
  
  # Do not use "localhost" here
  Address = bacula-sd-local                # N.B. Use a fully qualified name here
  SDPort = 9103
  Password = "TS8EQJ99iLFSK39oJy33YqkZ98v4ZapjRcA+j1N6ED1n"

  # CloudAutoChangerS3
  Device = "LocalCharge"

  Media Type = "CloudType"
  Maximum Concurrent Jobs = 10        # run up to 10 jobs a the same time
}
